<!doctype html>
<html lang="en-us">
    <head>
        <title>stream : Real-time Whisper transcription in WebAssembly</title>

        <style>
            #output {
                width: 100%;
                height: 100%;
                margin: 0 auto;
                margin-top: 10px;
                border-left: 0px;
                border-right: 0px;
                padding-left: 0px;
                padding-right: 0px;
                display: block;
                background-color: black;
                color: white;
                font-size: 10px;
                font-family: 'Lucida Console', Monaco, monospace;
                outline: none;
                white-space: pre;
                overflow-wrap: normal;
                overflow-x: scroll;
            }
        </style>
         <script src="https://code.jquery.com/jquery-3.6.4.min.js"
         integrity="sha256-oP6HI9z1XaZNBrJURtCoUT5SUnxFr8s3BzRl+cbzUq8="
         crossorigin="anonymous"></script>

         <meta http-equiv="origin-trial" content="Atny3N60DQ2VmUDTOosUDLiwpTRbuSgMZGV3jOdpXNLLCfHG/RDbIvsdWeWrwWeKfI2n3AaWF6xiUHQmgMzuugYAAACBeyJvcmlnaW4iOiJodHRwczovL3NraWxsaW5nc2hhcmsuZ2l0aHViLmlvOjQ0MyIsImZlYXR1cmUiOiJVbnJlc3RyaWN0ZWRTaGFyZWRBcnJheUJ1ZmZlciIsImV4cGlyeSI6MTcwOTg1NTk5OSwiaXNTdWJkb21haW4iOnRydWV9">
         <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.js"></script>
         <script src="https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.7/dist/bundle.min.js"></script>
<!--          <script >if (!crossOriginIsolated) SharedArrayBuffer = ArrayBuffer</script> -->
    </head>
    <body>       
        <div id="status">Recording not started</div>
        <div id="listening"></div>

        <div id="main-container">
            <b>stream : Real-time Whisper transcription in WebAssembly</b>

            <br><br>

            You can find more about this project on <a href="https://github.com/ggerganov/whisper.cpp/tree/master/examples/stream.wasm">GitHub</a>.

            <br><br>

            <b>More examples:</b>
                <a href="https://whisper.ggerganov.com/">main</a> |
                <a href="https://whisper.ggerganov.com/bench">bench</a> |
                <a href="https://whisper.ggerganov.com/stream">stream</a> |
                <a href="https://whisper.ggerganov.com/command">command</a> |
                <a href="https://whisper.ggerganov.com/talk">talk</a> |

            <br><br>

            <hr>

            Select the model you would like to use, click the "Start" button and start speaking

            <br><br>

            <div id="model-whisper">
                Whisper model: <span id="model-whisper-status"></span>
                <button id="fetch-whisper-tiny-en" onclick="loadWhisper('tiny.en')">tiny.en (75 MB)</button>
                <button id="fetch-whisper-base-en" onclick="loadWhisper('base.en')">base.en (142 MB)</button>
                <br><br>
                Quantized models:<br><br>
                <button id="fetch-whisper-tiny-en-q5_1"   onclick="loadWhisper('tiny-en-q5_1')">tiny.en (Q5_1, 31 MB)</button>
                <button id="fetch-whisper-base-en-q5_1"   onclick="loadWhisper('base-en-q5_1')">base.en (Q5_1, 57 MB)</button>
                <span id="fetch-whisper-progress"></span>

                <!--
                    <input type="file" id="file" name="file" onchange="loadFile(event, 'whisper.bin')" />
                -->
            </div>

            <br>

            <!-- <div id="input">
                <button id="start"  onclick="onStart()" disabled>Start</button>
                <button id="stop"   onclick="onStop()" disabled>Stop</button>
                <button id="clear"  onclick="clearCache()">Clear Cache</button>
            </div> -->

            <br>

            <div id="state">
                Status: <b><span id="state-status">not started</span></b>

                <pre id="state-transcribed">[The transcribed text will be displayed here]</pre>
            </div>

            <hr>

            Debug output:
            <textarea id="output" rows="20"></textarea>

            <br>

            <b>Troubleshooting</b>

            <br><br>

            The page does some heavy computations, so make sure:

            <ul>
                <li>To use a modern web browser (e.g. Chrome, Firefox)</li>
                <li>To use a fast desktop or laptop computer (i.e. not a mobile phone)</li>
                <li>Your browser supports WASM <a href="https://webassembly.org/roadmap/">Fixed-width SIMD</a></li>
            </ul>

            <div class="cell-version">
                <span>
                    |
                    Build time: <span class="nav-link">Sun Dec 10 23:17:52 2023</span> |
                    Commit hash: <a class="nav-link" href="https://github.com/ggerganov/whisper.cpp/commit/6335933a">6335933a</a> |
                    Commit subject: <span class="nav-link">cmake : Fix bug in httplib.h for mingw (#1615)</span> |
                    <a class="nav-link" href="https://github.com/ggerganov/whisper.cpp/tree/master/examples/stream.wasm">Source Code</a> |
                </span>
            </div>
        </div>


        <script type="text/javascript" src="helpers.js"></script>
        <script type='text/javascript'>
            // web audio context
            var context = null;

            // audio data
            var audio = null;
            var audio0 = null;

            // the stream instance
            var instance = null;

            // model name
            var model_whisper = null;

            var Module = {
                print: printTextarea,
                printErr: printTextarea,
                setStatus: function(text) {
                    printTextarea('js: ' + text);
                },
                monitorRunDependencies: function(left) {
                },
                preRun: function() {
                    printTextarea('js: Preparing ...');
                },
                postRun: function() {
                    printTextarea('js: Initialized successfully!');
                },
            };

            //
            // fetch models
            //

            let dbVersion = 1
            let dbName    = 'whisper.ggerganov.com';
            let indexedDB = window.indexedDB || window.mozIndexedDB || window.webkitIndexedDB || window.msIndexedDB

            function storeFS(fname, buf) {
                // write to WASM file using FS_createDataFile
                // if the file exists, delete it
                try {
                    Module.FS_unlink(fname);
                    // FS.unlink(fname);
                } catch (e) {
                    // ignore
                   
                }

                Module.FS_createDataFile("/", fname, buf, true, true);
                // FS.createDataFile("/", fname, buf, true, true);


                printTextarea('storeFS: stored model: ' + fname + ' size: ' + buf.length);

                document.getElementById('model-whisper-status').innerHTML = 'loaded "' + model_whisper + '"!';

                // if (model_whisper != null) {
                //     document.getElementById('start').disabled = false;
                //     document.getElementById('stop' ).disabled = true;
                // }
            }

            function loadWhisper(model) {
                let urls = {
                    'tiny.en': 'https://whisper.ggerganov.com/ggml-model-whisper-tiny.en.bin',
                    'base.en': 'http://127.0.0.1:5872/03_vad_wasm/ggml-model-whisper-base.en.bin',

                    'tiny-en-q5_1':  'https://whisper.ggerganov.com/ggml-model-whisper-tiny.en-q5_1.bin',
                    'base-en-q5_1':  'https://whisper.ggerganov.com/ggml-model-whisper-base.en-q5_1.bin',
                };

                let sizes = {
                    'tiny.en': 75,
                    'base.en': 142,

                    'tiny-en-q5_1':   31,
                    'base-en-q5_1':   57,
                };

                let url     = urls[model];
                let dst     = 'whisper.bin';
                let size_mb = sizes[model];

                model_whisper = model;

                document.getElementById('fetch-whisper-tiny-en').style.display = 'none';
                document.getElementById('fetch-whisper-base-en').style.display = 'none';

                document.getElementById('fetch-whisper-tiny-en-q5_1').style.display = 'none';
                document.getElementById('fetch-whisper-base-en-q5_1').style.display = 'none';

                document.getElementById('model-whisper-status').innerHTML = 'loading "' + model + '" ... ';

                cbProgress = function(p) {
                    let el = document.getElementById('fetch-whisper-progress');
                    el.innerHTML = Math.round(100*p) + '%';
                };

                cbCancel = function() {
                    var el;
                    // el = document.getElementById('fetch-whisper-tiny-en'); if (el) el.style.display = 'inline-block';
                    // el = document.getElementById('fetch-whisper-base-en'); if (el) el.style.display = 'inline-block';

                    // el = document.getElementById('fetch-whisper-tiny-en-q5_1'); if (el) el.style.display = 'inline-block';
                    // el = document.getElementById('fetch-whisper-base-en-q5_1'); if (el) el.style.display = 'inline-block';

                    el = document.getElementById('model-whisper-status');  if (el) el.innerHTML = '';
                };

                loadRemote(url, dst, size_mb, cbProgress, storeFS, cbCancel, printTextarea);
            }

            //
            // microphone
            //

            const kSampleRate = 16000;
            const kRestartRecording_s = 120;
            const kIntervalAudio_ms = 5000; // pass the recorded audio to the C++ instance at this rate

            var mediaRecorder = null;
            var doRecording = false;
            var startTime = 0;

            window.AudioContext = window.AudioContext || window.webkitAudioContext;
            window.OfflineAudioContext = window.OfflineAudioContext || window.webkitOfflineAudioContext;

            function stopRecording() {
                Module.set_status("paused");
                doRecording = false;
                audio0 = null;
                audio = null;
                context = null;
            }

            // function startRecording() {
            //     if (!context) {
            //         context = new AudioContext({
            //             sampleRate: kSampleRate,
            //             channelCount: 1,
            //             echoCancellation: false,
            //             autoGainControl:  true,
            //             noiseSuppression: true,
            //         });
            //     }

               

            //     navigator.mediaDevices.getUserMedia({audio: true, video: false})
            //         .then(function(s) {
            //             stream = s;
            //             mediaRecorder = new MediaRecorder(stream);
            //             mediaRecorder.ondataavailable = function(e) {
            //                 chunks.push(e.data);

            //                 var blob = new Blob(chunks, { 'type' : 'audio/ogg; codecs=opus' });
            //                 var reader = new FileReader();

            //                 reader.onload = function(event) {
            //                     var buf = new Uint8Array(reader.result);

            //                     if (!context) {
            //                         return;
            //                     }
            //                     context.decodeAudioData(buf.buffer, function(audioBuffer) {
            //                         var offlineContext = new OfflineAudioContext(audioBuffer.numberOfChannels, audioBuffer.length, audioBuffer.sampleRate);
            //                         var source = offlineContext.createBufferSource();
            //                         source.buffer = audioBuffer;
            //                         source.connect(offlineContext.destination);
            //                         source.start(0);

            //                         offlineContext.startRendering().then(function(renderedBuffer) {
            //                             audio = renderedBuffer.getChannelData(0);

            //                             //printTextarea('js: audio recorded, size: ' + audio.length + ', old size: ' + (audio0 == null ? 0 : audio0.length));

            //                             var audioAll = new Float32Array(audio0 == null ? audio.length : audio0.length + audio.length);
            //                             if (audio0 != null) {
            //                                 audioAll.set(audio0, 0);
            //                             }
            //                             audioAll.set(audio, audio0 == null ? 0 : audio0.length);

            //                             if (instance) {
            //                                 Module.set_audio(instance, audioAll);
            //                             }
            //                         });
            //                     }, function(e) {
            //                         audio = null;
            //                     });
            //                 }

            //                 reader.readAsArrayBuffer(blob);
            //             };

            //             mediaRecorder.onstop = function(e) {
            //                 if (doRecording) {
            //                     setTimeout(function() {
            //                         startRecording();
            //                     });
            //                 }
            //             };

            //             mediaRecorder.start(kIntervalAudio_ms);
            //         })

            //         .catch(function(err) {
            //             printTextarea('js: error getting audio stream: ' + err);
            //         });

            //     var interval = setInterval(function() {
            //         if (!doRecording) {
            //             clearInterval(interval);
            //             mediaRecorder.stop();
            //             stream.getTracks().forEach(function(track) {
            //                 track.stop();
            //             });

            //             // document.getElementById('start').disabled = false;
            //             // document.getElementById('stop').disabled  = true;

            //             mediaRecorder = null;
            //         }

            //         // if audio length is more than kRestartRecording_s seconds, restart recording
            //         if (audio != null && audio.length > kSampleRate*kRestartRecording_s) {
            //             if (doRecording) {
            //                 //printTextarea('js: restarting recording');

            //                 clearInterval(interval);
            //                 audio0 = audio;
            //                 audio = null;
            //                 mediaRecorder.stop();
            //                 stream.getTracks().forEach(function(track) {
            //                     track.stop();
            //                 });
            //             }
            //         }
            //     }, 100);
            // }

            //
            // main
            //

            
            ////////////////////////////////
            var nLines = 0;
            var intervalUpdate = null;
            var transcribedAll = '';
            
        async function main() {
        const statusElement = document.getElementById('status');
        const resultContainer = document.getElementById('resultContainer');
        const transcriptionResult = document.getElementById('transcriptionResult');
        const detectedLanguage = document.getElementById('detectedLanguage');

        // ////////////////////////////////////////
        // statusElement.textContent = 'Recording started';
        //         if (!instance) {
        //             instance = Module.init('whisper.bin');

        //             if (instance) {
        //                 printTextarea("js: whisper initialized, instance: " + instance);
        //             }
        //         }

        //         if (!instance) {
        //             printTextarea("js: failed to initialize whisper");
        //             return;
        //         }

        //         startRecording();

        //         intervalUpdate = setInterval(function() {
        //             var transcribed = Module.get_transcribed();

        //             if (transcribed != null && transcribed.length > 1) {
        //                 transcribedAll += transcribed + '<br>';
        //                 nLines++;

        //                 // if more than 10 lines, remove the first line
        //                 if (nLines > 10) {
        //                     var i = transcribedAll.indexOf('<br>');
        //                     if (i > 0) {
        //                         transcribedAll = transcribedAll.substring(i + 4);
        //                         nLines--;
        //                     }
        //                 }
        //             }

        //             document.getElementById('state-status').innerHTML = Module.get_status();
        //             document.getElementById('state-transcribed').innerHTML = transcribedAll;
        //         }, 100);


        //////////////////////////////////////////

        // let all_text = "";

        const myvad = await vad.MicVAD.new({

            onSpeechStart: () => {
                statusElement.textContent = 'Recording started';
                

                // startRecording();

                // intervalUpdate = setInterval(function() {
                //     var transcribed = Module.get_transcribed();

                //     if (transcribed != null && transcribed.length > 1) {
                //         transcribedAll += transcribed + '<br>';
                //         nLines++;

                //         // if more than 10 lines, remove the first line
                //         if (nLines > 10) {
                //             var i = transcribedAll.indexOf('<br>');
                //             if (i > 0) {
                //                 transcribedAll = transcribedAll.substring(i + 4);
                //                 nLines--;
                //             }
                //         }
                //     }

                //     document.getElementById('state-status').innerHTML = Module.get_status();
                //     document.getElementById('state-transcribed').innerHTML = transcribedAll;
                // }, 100);

            },
        
            
            onSpeechEnd: (audio) => {
                if (!instance) {
                    instance = Module.init('whisper.bin');

                    if (instance) {
                        printTextarea("js: whisper initialized, instance: " + instance);
                    }
                }

                if (!instance) {
                    printTextarea("js: failed to initialize whisper");
                    return;
                }
                // const wavBuffer = vad.utils.encodeWAV(arr)
                // const base64 = vad.utils.arrayBufferToBase64(wavBuffer)
                statusElement.textContent = 'Recording finished';
                // stopRecording();
                var audioAll = new Float32Array(audio0 == null ? audio.length : audio0.length + audio.length);
                            if (audio0 != null) {
                                audioAll.set(audio0, 0);
                            }
                            audioAll.set(audio, audio0 == null ? 0 : audio0.length);

                            if (instance) {
                                Module.set_audio(instance, audioAll);
                            }
//// start recording called
if (!context) {
                    context = new AudioContext({
                        sampleRate: kSampleRate,
                        channelCount: 1,
                        echoCancellation: false,
                        autoGainControl:  true,
                        noiseSuppression: true,
                    });
                }
                            Module.set_status("");

                            doRecording = true;
                            startTime = Date.now();

                            var chunks = [];
                            var stream = null;

                            //// start recording ended
                    
                intervalUpdate = setInterval(function() {
                    var transcribed = Module.get_transcribed();

                    if (transcribed != null && transcribed.length > 1) {
                        transcribedAll += transcribed + '<br>';
                        nLines++;

                        // if more than 10 lines, remove the first line
                        if (nLines > 10) {
                            var i = transcribedAll.indexOf('<br>');
                            if (i > 0) {
                                transcribedAll = transcribedAll.substring(i + 4);
                                nLines--;
                            }
                        }
                    }

                    document.getElementById('state-status').innerHTML = Module.get_status();
                    // document.getElementById('state-transcribed').innerHTML = transcribedAll;
                }, 100);
                stopRecording();
                       console.log('transciption //////////////')      
                console.log(transcribedAll)
                searchString = "Hello"
                if (transcribedAll.includes(searchString)) 
                {   
                    console.log('got it ');
                    console.log('got it ');
                    console.log('got it ');
                    console.log('got it ');

                    secondary()
                }
                else if (transcribedAll.includes("hello"))
                {
                    console.log('got it ');
                    console.log('got it ');
                    console.log('got it ');
                    console.log('got it ');

                    secondary()
                }
                else if (transcribedAll.includes("Okay"))
                {
                    console.log('got it ');
                    console.log('got it ');
                    console.log('got it ');
                    console.log('got it ');

                    secondary()
                }
                // searchString= "Okay"
                // stopRecording();
                // else 
                // {
                //     console.log("The text does not contain the specified string.");
                // }


            },
        });

        myvad.start();
        // const listeningstatus = document.getElementById('listening');
        // listeningstatus.textContent = myvad.listening();

    }
        
    async function secondary()
    {   
        var cnt = 0 ;
        const statusElement = document.getElementById('status');
        const resultContainer = document.getElementById('resultContainer');
        const transcriptionResult = document.getElementById('transcriptionResult');
        const detectedLanguage = document.getElementById('detectedLanguage');


        const myvad = await vad.MicVAD.new({

                onSpeechStart: () => {
                    statusElement.textContent = 'Recording started';
                        },


                onSpeechEnd: (audio) => {
                    cnt = 1;
                    if (!instance) {
                        instance = Module.init('whisper.bin');

                        if (instance) {
                            printTextarea("js: whisper initialized, instance: " + instance);
                        }
                    }

                    if (!instance) {
                        printTextarea("js: failed to initialize whisper");
                        return;
                    }
                    // const wavBuffer = vad.utils.encodeWAV(arr)
                    // const base64 = vad.utils.arrayBufferToBase64(wavBuffer)
                    statusElement.textContent = 'Recording finished';
                    // stopRecording();
                    var audioAll = new Float32Array(audio0 == null ? audio.length : audio0.length + audio.length);
                                if (audio0 != null) {
                                    audioAll.set(audio0, 0);
                                }
                                audioAll.set(audio, audio0 == null ? 0 : audio0.length);

                                if (instance) {
                                    Module.set_audio(instance, audioAll);
                                }
                //// start recording called
                if (!context) {
                        context = new AudioContext({
                            sampleRate: kSampleRate,
                            channelCount: 1,
                            echoCancellation: false,
                            autoGainControl:  true,
                            noiseSuppression: true,
                        });
                    }
                                Module.set_status("");

                                doRecording = true;
                                startTime = Date.now();

                                var chunks = [];
                                var stream = null;

                                // start recording ended
                        
                    intervalUpdate = setInterval(function() {
                        var transcribed = Module.get_transcribed();

                        if (transcribed != null && transcribed.length > 1) {
                            transcribedAll += transcribed + '<br>';
                            nLines++;

                            // if more than 10 lines, remove the first line
                            if (nLines > 10) {
                                var i = transcribedAll.indexOf('<br>');
                                if (i > 0) {
                                    transcribedAll = transcribedAll.substring(i + 4);
                                    nLines--;
                                }
                            }
                        }

                        document.getElementById('state-status').innerHTML = Module.get_status();
                        document.getElementById('state-transcribed').innerHTML = transcribedAll;
                    }, 100);
                    stopRecording();         
                    
                    // searchString = "Hello"
                    // if (transcribedAll.includes(searchString)) 
                    // {
                    //     secondary()
                    // }

                    if(cnt == 1)
                    {   
                        console.log(transcribedAll);
                        main()
                        // return ;
                    }

                    // else 
                    // {
                    //     console.log("The text does not contain the specified string.");
                    // }


                },
                });

                myvad.start();
                // const listeningstatus = document.getElementById('listening');
                // listeningstatus.textContent = myvad.listening();


    }
    main();


            /////////////////////////////////

        </script>
        <script type="text/javascript" src="stream.js"></script>
    </body>
</html>
