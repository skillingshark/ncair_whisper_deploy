<!doctype html>
<html lang="en-us">
    <head>
        <title>stream : Real-time Whisper transcription in WebAssembly</title>
        <link rel="stylesheet" href="styles.css">
         <script src="https://code.jquery.com/jquery-3.6.4.min.js"
         integrity="sha256-oP6HI9z1XaZNBrJURtCoUT5SUnxFr8s3BzRl+cbzUq8="
         crossorigin="anonymous"></script>
        <!--  TOKEN FOR GITHUB -->
         <meta http-equiv="origin-trial" content="Atny3N60DQ2VmUDTOosUDLiwpTRbuSgMZGV3jOdpXNLLCfHG/RDbIvsdWeWrwWeKfI2n3AaWF6xiUHQmgMzuugYAAACBeyJvcmlnaW4iOiJodHRwczovL3NraWxsaW5nc2hhcmsuZ2l0aHViLmlvOjQ0MyIsImZlYXR1cmUiOiJVbnJlc3RyaWN0ZWRTaGFyZWRBcnJheUJ1ZmZlciIsImV4cGlyeSI6MTcwOTg1NTk5OSwiaXNTdWJkb21haW4iOnRydWV9">
         <!-- TOKEN FOR PORT 5872 -->
         <!-- <meta http-equiv="origin-trial" content="AtL1nQI0P4o0J+7BIUnXmUZbTKAzhbw61i7rG+bnhTOBdptUUsHShOokVnusoHtmIjX7gUXzGL8DvG4SRZz1uA8AAABgeyJvcmlnaW4iOiJodHRwOi8vMTI3LjAuMC4xOjU4NzIiLCJmZWF0dXJlIjoiVW5yZXN0cmljdGVkU2hhcmVkQXJyYXlCdWZmZXIiLCJleHBpcnkiOjE3MDk4NTU5OTl9"> -->

         <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.js"></script>
         <script src="https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.7/dist/bundle.min.js"></script>
         <script type="text/javascript" src="https://cdn.skypack.dev/d3@7"></script>
        
    </head>
    <body>       
        <div id="status">Recording not started</div>
        <div id="listening"></div>

        <div id="main-container">
            <b>STREAM : Real-time Whisper transcription in WebAssembly</b>

            <br><br>
            <p id="welcome">Select the model you would like to use .... You can start speaking Once the model is initialized</p>    
            

            <br><br>
            
            <br><br>

            <div id="model-whisper">

               <p class="model-type"id="whisper-model-label" >Whisper model: </p> <span id="model-whisper-status"></span> <br>

                <button id="fetch-whisper-tiny-en" onclick="loadWhisper('tiny.en')">tiny.en (75 MB)</button>
                <button id="fetch-whisper-base-en" onclick="loadWhisper('base.en')">base.en (142 MB)</button>
                <br><br>

                <p class="model-type" id="quantized-model-label">Quantized models:</p>

                <button id="fetch-whisper-tiny-en-q5_1"   onclick="loadWhisper('tiny-en-q5_1')">tiny.en (Q5_1, 31 MB)</button>
                <button id="fetch-whisper-base-en-q5_1"   onclick="loadWhisper('base-en-q5_1')">base.en (Q5_1, 57 MB)</button>

                <span id="fetch-whisper-progress"></span>

                <!--
                    <input type="file" id="file" name="file" onchange="loadFile(event, 'whisper.bin')" />
                -->
            </div>

            <br>

            <br> 
            <br><br><br><br><br>

            <div id="state">
                Status: <b><span id="state-status">not started</span></b>

                <pre id="state-transcribed">[The transcribed text will be displayed here]</pre>
            </div>

            <hr>

            <div id="greeting-container">
                <p id="greeting-text">Yes, Sir! What can I do for you?</p>
            </div>



            <br>

            <br><br>
        </div>


        <script type="text/javascript" src="helpers.js"></script>
        <script type="text/javascript" src="https://cdn.skypack.dev/d3-scale-chromatic@3"></script>
        <script type="text/javascript">
            // <script type="module">import { interpolateInferno } from "https://cdn.skypack.dev/d3-scale-chromatic@3"
                // const output_area = document.getElementById('state-transcribed');

            // web audio context
            var context = null;

            // audio data
            var audio = null;
            var audio0 = null;

            // the stream instance
            var instance = null;

            // model name
            var model_whisper = null;

            var Module = {
                print: printTextarea,
                // print: printTextarea2,
                printErr: printTextarea,
                setStatus: function(text) {
                    printTextarea('js: ' + text);
                },
                monitorRunDependencies: function(left) {
                },
                preRun: function() {
                    printTextarea('js: Preparing ...');
                },
                postRun: function() {
                    printTextarea('js: Initialized successfully!');
                },
            };

            //
            // fetch models
            //

            let dbVersion = 1
            let dbName    = 'whisper.ggerganov.com';
            let indexedDB = window.indexedDB || window.mozIndexedDB || window.webkitIndexedDB || window.msIndexedDB

            function storeFS(fname, buf) {
                // write to WASM file using FS_createDataFile
                // if the file exists, delete it
                try {
                    Module.FS_unlink(fname);
                    // FS.unlink(fname);
                } catch (e) {
                    // ignore
                   
                }

                Module.FS_createDataFile("/", fname, buf, true, true);
                // FS.createDataFile("/", fname, buf, true, true);


                printTextarea('storeFS: stored model: ' + fname + ' size: ' + buf.length);

                document.getElementById('model-whisper-status').innerHTML = 'Model Loaded :  "' + model_whisper + '"';
            }

            function loadWhisper(model) {
                document.getElementById('model-whisper-status').classList.add('highlighted');
                document.getElementById('welcome').style.display = 'none';
                let urls = {
                    'tiny.en': './ggml-model-whisper-tiny.en.bin',
                    'base.en': './ggml-model-whisper-base.en.bin',

                    'tiny-en-q5_1':  'https://whisper.ggerganov.com/ggml-model-whisper-tiny.en-q5_1.bin',
                    'base-en-q5_1':  'https://whisper.ggerganov.com/ggml-model-whisper-base.en-q5_1.bin',
                };

                let sizes = {
                    'tiny.en': 75,
                    'base.en': 142,

                    'tiny-en-q5_1':   31,
                    'base-en-q5_1':   57,
                };

                let url     = urls[model];
                let dst     = 'whisper.bin';
                let size_mb = sizes[model];

                model_whisper = model;

                document.getElementById('whisper-model-label').style.display = 'none';
                document.getElementById('quantized-model-label').style.display = 'none';
                document.getElementById('fetch-whisper-tiny-en').style.display = 'none';
                document.getElementById('fetch-whisper-base-en').style.display = 'none';

                document.getElementById('fetch-whisper-tiny-en-q5_1').style.display = 'none';
                document.getElementById('fetch-whisper-base-en-q5_1').style.display = 'none';

                document.getElementById('model-whisper-status').innerHTML = 'loading "' + model + '" ... ';

                cbProgress = function(p) {
                    let el = document.getElementById('fetch-whisper-progress');
                    el.innerHTML = Math.round(100*p) + '%';
                };

                cbCancel = function() {
                    var el
                    el = document.getElementById('model-whisper-status');  if (el) el.innerHTML = '';
                };

                loadRemote(url, dst, size_mb, cbProgress, storeFS, cbCancel, printTextarea);
            }

            //
            // microphone
            //

            const kSampleRate = 16000;
            const kRestartRecording_s = 120;
            const kIntervalAudio_ms = 5000; // pass the recorded audio to the C++ instance at this rate

            var mediaRecorder = null;
            var doRecording = false;
            var startTime = 0;

            window.AudioContext = window.AudioContext || window.webkitAudioContext;
            window.OfflineAudioContext = window.OfflineAudioContext || window.webkitOfflineAudioContext;

            function stopRecording() {
                Module.set_status("paused");
                doRecording = false;
                audio0 = null;
                audio = null;
                context = null;
            }


            
            ////////////////////////////////
            var nLines = 0;
            var intervalUpdate = null;
            var transcribedAll = '';
        //   import { interpolateInferno } from "https://cdn.skypack.dev/d3-scale-chromatic@3" 
        
        
        function showGreeting() 
        {
            // Show greeting container
            const greetingContainer = document.getElementById('greeting-container');
            greetingContainer.classList.add('show');

            // Hide greeting after 3 seconds (adjust as needed)
            // setTimeout(() => {
            //     greetingContainer.classList.remove('show');
            // }, 3000);
        }     
        async function main() 
        {

        console.log('entered MAIN ////////////////////');
        // transcribedAll = '';

        const statusElement = document.getElementById('status');
        const resultContainer = document.getElementById('resultContainer');
        const transcriptionResult = document.getElementById('transcriptionResult');
        const detectedLanguage = document.getElementById('detectedLanguage');

        //////////////////////////////////////////

        const myvad = await vad.MicVAD.new({

            onSpeechStart: () =>{
                statusElement.textContent = 'Recording started';
            },

            // onFrameProcessed: (probs) => {
            //     gradient();
            // },
            
            onSpeechEnd: (audio) => {
                if (!instance) {
                    instance = Module.init('whisper.bin');

                    if (instance) {
                        printTextarea("js: whisper initialized, instance: " + instance);
                    }
                }

                if (!instance) {
                    printTextarea("js: failed to initialize whisper");
                    return;
                }
                // const wavBuffer = vad.utils.encodeWAV(arr)
                // const base64 = vad.utils.arrayBufferToBase64(wavBuffer)
                statusElement.textContent = 'Recording finished';
                // stopRecording();
                var audioAll = new Float32Array(audio0 == null ? audio.length : audio0.length + audio.length);
                            if (audio0 != null) {
                                audioAll.set(audio0, 0);
                            }
                            audioAll.set(audio, audio0 == null ? 0 : audio0.length);

                            if (instance) {
                                Module.set_audio(instance, audioAll);
                            }
                //// start recording called
                if (!context) {
                                    context = new AudioContext({
                                        sampleRate: kSampleRate,
                                        channelCount: 1,
                                        echoCancellation: false,
                                        autoGainControl:  true,
                                        noiseSuppression: true,
                                    });
                                }
                                            Module.set_status("");

                                            doRecording = true;
                                            startTime = Date.now();

                                            var chunks = [];
                                            var stream = null;

                                            //// start recording ended
                                    
                                intervalUpdate = setInterval(function() {
                                    var transcribed = Module.get_transcribed();

                                    if (transcribed != null && transcribed.length > 1) {
                                        transcribedAll += transcribed + '\n';
                                        nLines++;

                                        // if more than 10 lines, remove the first line
                                        if (nLines > 10) {
                                            var i = transcribedAll.indexOf('\n');
                                            if (i > 0) {
                                                transcribedAll = transcribedAll.substring(i + 4);
                                                nLines--;
                                            }
                                        }
                                    }

                                    document.getElementById('state-status').innerHTML = Module.get_status();
                                    // document.getElementById('state-transcribed').innerHTML = transcribedAll;
                                }, 100);
                                        stopRecording();
                                        console.log('recording stopped in main')      
                                        console.log(transcribedAll)
                                searchString = ""
                                if (transcribedAll.toLowerCase().includes("ok twin"))
                                {
                                    console.log('got it ');
                                    console.log('got it ');
                                    console.log('got it ');
                                    console.log('got it ');

                                    // transcribedAll = '';
                                    showGreeting();
                                    secondary();
                                }
                                else if (transcribedAll.toLowerCase().includes("hello"))
                                {
                                    console.log('got it ');
                                    console.log('got it ');
                                    console.log('got it ');
                                    console.log('got it ');
                                    showGreeting();
                                    secondary();
                                }
                                // else if (transcribedAll.toLowerCase().includes("google"))
                                // {
                                //     console.log('got it ');
                                //     console.log('got it ');
                                //     console.log('got it ');
                                //     console.log('got it ');

                                //     secondary()
                                // }
            },
        });

        myvad.start();

    }
        
    async function secondary()
    {   
        console.log("entered secondary //////////////////////")
        transcribedAll = '';
        var cnt = 0 ;
        const statusElement = document.getElementById('status');
        const resultContainer = document.getElementById('resultContainer');
        const transcriptionResult = document.getElementById('transcriptionResult');
        const detectedLanguage = document.getElementById('detectedLanguage');


        const myvad = await vad.MicVAD.new({

                onSpeechStart: () => {
                    statusElement.textContent = 'Recording started INSIDE SECONDARY';
                        },


                onSpeechEnd: (audio) => {
                    cnt = 1;
                    if (!instance) {
                        instance = Module.init('whisper.bin');

                        if (instance) {
                            printTextarea("js: whisper initialized, instance: " + instance);
                        }
                    }

                    if (!instance) {
                        printTextarea("js: failed to initialize whisper");
                        return;
                    }
                    // const wavBuffer = vad.utils.encodeWAV(arr)
                    // const base64 = vad.utils.arrayBufferToBase64(wavBuffer)
                    statusElement.textContent = 'Recording finished INSIDE SECONDARY';
                    // stopRecording();
                    var audioAll = new Float32Array(audio0 == null ? audio.length : audio0.length + audio.length);
                                if (audio0 != null) {
                                    audioAll.set(audio0, 0);
                                }
                                audioAll.set(audio, audio0 == null ? 0 : audio0.length);

                                if (instance) {
                                    Module.set_audio(instance, audioAll);
                                }
                //// start recording called
                if (!context) {
                        context = new AudioContext({
                            sampleRate: kSampleRate,
                            channelCount: 1,
                            echoCancellation: false,
                            autoGainControl:  true,
                            noiseSuppression: true,
                        });
                    }
                                Module.set_status("");

                                doRecording = true;
                                startTime = Date.now();

                                var chunks = [];
                                var stream = null;

                                // start recording ended
                        
                    intervalUpdate = setInterval( function() 
                    {
                        var transcribed = Module.get_transcribed();

                        if (transcribed != null && transcribed.length > 1) {
                            transcribedAll += transcribed + '\n';
                            nLines++;

                            // if more than 10 lines, remove the first line
                            if (nLines > 10) {
                                var i = transcribedAll.indexOf('\n');
                                if (i > 0) {
                                    transcribedAll = transcribedAll.substring(i + 4);
                                    nLines--;
                                }
                            }
                        }

                        document.getElementById('state-status').innerHTML = Module.get_status();
                        console.log('time to print transcribedAll');
                        printTextarea(transcribedAll);
                        console.log(transcribedAll);
                        document.getElementById('state-transcribed').textContent = transcribedAll;
                    }, 100);
                    stopRecording();         


                },
                });

                myvad.start();
                if(cnt == 1)
                    {   
                        console.log('printing from secondary')
                        console.log(transcribedAll);
                        main()
                        // return ;
                    }


    }
    main();


            /////////////////////////////////

        </script>
        <script type="text/javascript" src="stream.js"></script>
    </body>
</html>
